apiVersion: cloudnativetoolkit.dev/v2
kind: Solution
metadata:
  name: infra-aws-standard
  labels:
    type: infrastructure
    platform: aws
    flavor: standard
  annotations:
    displayName: 'Infrastructure: AWS Standard'
    description: >-
      A standard production deployment environment with typical security
      protections, private endpoints, VPN server, key management encryption,
      ideal for POC/POT/MVP.
    files.cloudnativetoolkit.dev/diagram: diagram.png
    files.cloudnativetoolkit.dev/readme: README.md
spec:
  version: v1.0.0
  variables:
    - name: access_key
      type: string
    - name: config_banner_text
      type: string
      description: The text that will appear in the top banner in the cluster
    - name: gitops_repo_repo
      type: string
      description: >-
        The short name of the repository (i.e. the part after the org/group
        name)
    - name: secret_key
      type: string
      sensitive: true
    - name: vpn_authentication_saml_provider_arn
      type: string
      description: >-
        (Optional) The ARN of the IAM SAML identity provider if type is
        federated-authentication.
    - name: aws-ec2-bastion_ami_id
      type: string
      description: AMI ID for bastion host
      value: ''
    - name: aws-ec2-bastion_instance_type
      type: string
      description: EC2 Instance Type 2 default
      value: t2.micro
    - name: cluster_compute-machine-type
      type: string
      description: >-
        Instance type for the compute nodes. Determines the amount of memory and
        vCPU allocated to each compute node. Default m5.xlarge
      value: m5.xlarge
    - name: cluster_no_of_compute_nodes
      type: number
      description: Number of worker nodes to be provisioned
      value: 1
    - name: cluster_ocp_version
      type: string
      description: Version of OpenShift that will be used to install the cluster
      value: 4.9.15
    - name: gitops_repo_host
      type: string
      description: >-
        The host for the git repository. The git host used can be a GitHub,
        GitHub Enterprise, Gitlab, Bitbucket, Gitea or Azure DevOps server. If
        the host is null assumes in-cluster Gitea instance will be used.
      value: ''
    - name: gitops_repo_org
      type: string
      description: >-
        The org/group where the git repository exists/will be provisioned. If
        the value is left blank then the username org will be used.
      value: ''
    - name: gitops_repo_project
      type: string
      description: >-
        The project that will be used for the git repo. (Primarily used for
        Azure DevOps repos)
      value: ''
    - name: gitops_repo_token
      type: string
      description: The personal access token used to access the repository
      value: ''
      sensitive: true
    - name: gitops_repo_username
      type: string
      description: The username of the user with access to the repository
      value: ''
    - name: multi-zone
      type: bool
      description: Create subnets in multiple zones
      value: true
    - name: name_prefix
      type: string
      description: Prefix to be added to the names of resources which are being provisioned
      value: ''
    - name: ngw_connectivity_type
      type: string
      description: >-
        (Optional) Connectivity type for the gateway. Valid values are private
        and public. Defaults to public.
      value: public
    - name: provision
      type: bool
      description: >-
        Flag indicating that the instance should be provisioned. If false then
        an existing instance will be looked up
      value: true
    - name: region
      type: string
      description: The deployment region
      value: ap-south-1
    - name: resource_group_name
      type: string
      description: >-
        The name of the resource group where the VPC is deployed. On AWS this
        value becomes a tag.
      value: default
    - name: rosa_token
      type: string
      description: >-
        get an offline access token at
        https://cloud.redhat.com/openshift/token/rosa 
      value: ''
      sensitive: true
    - name: vpn_allowed_cidr_ranges
      type: list(string)
      description: List of CIDR ranges from which access is allowed.
      value:
        - 10.0.0.0/16
    - name: vpn_client_cidr_block
      type: string
      description: client cidr block for vpn
      value: 172.13.0.0/16
    - name: vpn_dns_servers
      type: list(string)
      description: List of DNS Servers.
      value:
        - 10.0.0.2
  files:
    - name: README.md
      type: doc
      content: >
        # AWS Cloud Reference Architecture - Automation


        Automation to provision the Standard reference architecture on AWS
        Cloud. This architecture implements the minimum infrastructure required
        to stand up a managed Red Hat OpenShift cluster with public endpoints.


        ## Reference Architecture


        ![Standard](aws_standard_architecture.png)


        The automation is delivered in a number of layers that are applied in
        order. Layer `110` provisions the infrastructure including the Red Hat
        OpenShift cluster and the remaining layers provide configuration inside
        the cluster. Each layer depends on resources provided in the layer
        before it (e.g. `200` depends on `110`). Where two layers have the same
        numbers (e.g. `205`), you have a choice of which layer to apply.


        <table>

        <thead>

        <tr>

        <th>Layer name</th>

        <th>Layer description</th>

        <th>Provided resources</th>

        </tr>

        </thead>

        <tbody>

        <tr>

        <td>110 - AWS VPC OpenShift</td>

        <td>This layer provisions the bulk of the AWS Cloud infrastructure</td>

        <td>

        <h4>Network</h4>

        <ul>

        <li>VPC network</li>

        <li>VPC Subnet</li>

        <li>VPC Public Gateways</li>

        <li>Red Hat OpenShift cluster</li>

        </ul>

        <h4>Shared Services</h4>

        <ul>

        <li>Object Storage</li>

        <li>Key Managment Service</li>

        <li>Monitoring</li>

        <ul>

        </ul>

        </td>

        </tr>

        <tr>

        <td>200 -  AWS OpenShift Gitops </td>

        <td>This layer provisions OpenShift CI/CD tools into the cluster, a
        GitOps repository, and bootstraps the repository to the OpenShift Gitops
        instance.</td>

        <td>

        <h4>Software </h4>

        <ul>

        <li>OpenShift GitOps (ArgoCD)</li>

        <li>OpenShift Pipelines (Tekton)</li>

        <li>Sealed Secrets (Kubeseal)</li>

        <li>GitOps repo</li>

        </ul>

        </td>

        </tr>

        <tr>

        <td>210 - AWS Storage</td>

        <td>The storage layer offers `portworx`. Portworx storage can be
        installed on ROSA cluster.

        </td>

        <td>

        <ul>    

        </ul>

        <h4>Portworx Storage</h4>

        <ul>

        <li>AWS Cloud storage volumes</li>

        <li>Portworx operator</li>

        <li>Portworx storage classes</li>

        </ul>

        </td>

        </tr>

        <tr>

        <td>220 - Dev Tools</td>

        <td>The dev tools layer installs standard continuous integration (CI)
        pipelines that integrate with tools that support the software
        development lifecycle.</td>

        <td>

        <h4>Software</h4>

        <ul>

        <li>Artifactory</li>

        <li>Developer Dashboard</li>

        <li>Pact Broker</li>

        <li>Sonarqube</li>

        <li>Tekton Resources</li>

        </ul>

        </td>

        </tr>

        </tbody>

        </table>


        ## Automation


        ### Prerequisites


        1. Have access to an AWS Cloud Account. An Enterprise account is best
        for workload isolation but this terraform can be run in a Pay Go account
        as well.


        2. (Optional) Install and start Docker to run the terraform tools in a
        local bootstrapped container image. In the below example we have shown
        install method for Mac. Refer to Docker portal for
        ([Windows](https://docs.docker.com/desktop/install/windows-install/))
        and ([Linux](https://docs.docker.com/desktop/install/linux-install/))
        Install procedure.

            ```shell
            brew install docker 
            ```

        ### Setup


        1. Clone this repository to your local SRE laptop or into a secure
        terminal. Open a shell into the cloned directory.

        2. Copy **credentials.template** to **credentials.properties**.
            ```shell
            cp credentials.template credentials.properties
            ```
        3. Provide values for the variables in **credentials.properties** 
        (**Note:** `*.properties` has been added to **.gitignore** to ensure
        that the file containing the apikey cannot be checked into Git. Do not
        use quotes around the values in properties file )
            - **TF_VAR_aws_access_key_id** - The API key for the AWS Cloud account where the infrastructure will be provisioned.
            - **TF_VAR_aws_secret_access_key** - The API key for the AWS Cloud account where the infrastructure will be provisioned.
            - **AWS_ACCESS_KEY_ID=** - The API key for the AWS Cloud account where the infrastructure will be provisioned.
            - **AWS_SECRET_ACCESS_KEY** - The API key for the AWS Cloud account where the infrastructure will be provisioned.
            - **TF_VAR_rosa_token** - The offline rosa token used to provision  ROSA cluster
            - **TF_VAR_gitops_repo_username** - The username on github.com that will be used to provision the gitops repository.
            - **TF_VAR_gitops_repo_token** - The personal access token that will be used to authenticate to github.com to provision the gitops repository. (The user should have necessary access in the org to create the repository and the token should have `delete_repo` permission.)
            - **TF_VAR_gitops_repo_org** - (Optional) The github.com org where the gitops repository will be provisioned. If not provided the org will default to the username. 
            - **TF_VAR_portworx_spec** - (Optional) ([Create Portworx Storage Spec](https://central.portworx.com/dashboard)).Kindly refer to following page for ([Portworx Storage Spec creation procedure](https://github.com/cloud-native-toolkit/terraform-aws-portworx/blob/main/PORTWORX_ESSENTIALS.md)).

        4. Run **./launch.sh**. This will start a container image with the
        prompt opened in the `/terraform` directory, pointed to the repo
        directory.

        5. Create a working copy of the terraform by running
        **./setup-workspace.sh**. . The script makes a copy of the terraform in
        /workspaces/current and set up "cluster.tfvars" and "gitops.tfvars"
        files populated with default values. The setup-workspace.sh script has a
        number of optional arguments.
            
            ```
            Usage: setup-workspace.sh [-f FLAVOR] [-s STORAGE] [-r REGION] [-n PREFIX_NAME] [-b BANNER_TEXT] [-g GIT_HOST] [-h HELP]
            
            options:
            -f   the type of deployment quickstart, standard or advanced. Make sure to use 'standard'
            -s   the storage option to use (portworx)
            -n   (optional) the name prefix that should be added to all the resources and length of prefix should not exceed 5 characters.  If not provided a prefix will not be added. 
            

            -r   (optional) the region where the infrastructure will be provisioned. 
                    Note: the AWS Cloud region where the infrastructure will be provided [available regions](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html). Codes for each location can be obtained from the CLI from shell - "aws ec2 describe-regions --output table". If this value is not provided then the value defaults to ap-south-1  (Note : User should always chose a AWS Region with minimum 3 AZs)
                      
            -b   (optional) customer message to display on OCP console as a banner
            -g   (optional) the git host that will be used for the gitops repo. If left blank gitea will be used by default. (Github, Github Enterprise ,  Gitlab, Bitbucket, Azure DevOps, and Gitea servers are supported)
            -h   Print this help    

            ```
        6. Change the directory to the current workspace where the automation
        was configured (e.g. `/workspaces/current`).

        7. Two different configuration files have been created:
        **cluster.tfvars** and **gitops.tfvars**. **cluster.tfvars** contains
        the variables specific to the infrastructure and cluster that will be
        provisioned. **gitops.tfvars** contains the variables that define the
        gitops configuration. Inspect both of these files to see if there are
        any variables that should be changed. (The **setup-workspace.sh** script
        has generated these two files with default values and can be used
        without updates, if desired.). E.g. cluster_ocp_version="4.9.15" can be
        changed to latest version cluster_ocp_version="4.10.01"


        #### Run the entire automation stack automatically


        From the **/workspace/current** directory, run the following:


        ```shell

        ./apply.sh

        ```


        The script will run through each of the terraform layers in sequence to
        provision the entire infrastructure.


        #### Run each of the Terraform layers manually


        From the **/workspace/current** directory, change the directory into
        each of the layer subdirectories, in order, and run the following:


        ```shell

        ./apply.sh

        ```


        ### Obtain login information


        Once the "110-aws-std-openshift-vpc"  has successfully run it is
        possible to obtain the login information by running from the
        **/workspace/current** directory:

        ```shell

        ./show-login.sh standard

        ```
    - name: diagram.png
      type: image
      contentUrl: >-
        https://raw.githubusercontent.com/IBM/automation-aws-infra-openshift/main/2-standard/aws_standard_architecture.png
  stack:
    - name: 110-aws-std-openshift-vpc
      layer: infrastructure
      description: Provision ROSA managed OpenShift on AWS
      version: v1.0.1
    - name: 200-openshift-gitops
      layer: software
      description: >-
        Provisions OpenShift GitOps (ArgoCD) into an existing cluster and
        bootstraps it to a gitops repository
      version: v1.0.1
    - name: 220-dev-tools
      layer: software
      description: Provisions development tools in an OpenShift cluster
      version: v1.0.0
